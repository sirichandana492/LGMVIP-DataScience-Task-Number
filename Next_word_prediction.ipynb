{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I3S4XyP6IYO0"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('dataset_book.txt') as file:\n",
        "    data=file.read()"
      ],
      "metadata": {
        "id": "9VTbgbGvIgII"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#     non_html=re.sub('<.*?>',' ',x)   #remove html tag using regular expresion\n",
        "soup=BeautifulSoup(data,'html.parser')#remove html tag using beautifulsoup\n",
        "non_html_text=soup.get_text()\n",
        "\n",
        "#remove unwanted charectors and symbols\n",
        "text=re.sub('[^a-zA-Z0-9\\s]',' ',non_html_text)\n",
        "text = text.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "#remove extra spaces\n",
        "z = []\n",
        "for i in text.split():\n",
        "    if i not in z:\n",
        "        z.append(i)\n",
        "text = ' '.join(z)\n",
        "\n",
        "#tokenize text\n",
        "\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "sequence=tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "sequence[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE6yB_NMIkE8",
        "outputId": "6dca57fc-c293-4feb-c48a-4b7d038e7dee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[39, 1, 40, 424, 425, 426, 2, 427, 428, 3, 429, 4, 41, 430, 42]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ip_dim=len(tokenizer.word_index)+1\n",
        "ip_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vAfThamIrGC",
        "outputId": "cb4c85a6-3e29-4496-f95f-beb319322fa8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6746"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#traning sequence\n",
        "\n",
        "sequences = []\n",
        "\n",
        "for i in range(3, len(sequence)):\n",
        "    words = sequence[i-3:i+1]\n",
        "    sequences.append(words)\n",
        "\n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "\n",
        "x,y=[],[]\n",
        "for i in sequences:\n",
        "    x.append(i[0:3])\n",
        "    y.append(i[3])\n",
        "x=np.array(x)\n",
        "y=np.array(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mdZavi3Itra",
        "outputId": "5e26ad1c-8dfd-42e1-992f-124b452e388c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of sequences are:  7203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:10],y[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzeYPVHXIvwa",
        "outputId": "9fe7ff38-cd2d-4a6c-f4e2-2dcad4a2ca99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 39,   1,  40],\n",
              "        [  1,  40, 424],\n",
              "        [ 40, 424, 425],\n",
              "        [424, 425, 426],\n",
              "        [425, 426,   2],\n",
              "        [426,   2, 427],\n",
              "        [  2, 427, 428],\n",
              "        [427, 428,   3],\n",
              "        [428,   3, 429],\n",
              "        [  3, 429,   4]]),\n",
              " array([424, 425, 426,   2, 427, 428,   3, 429,   4,  41]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# # # Pad \"sequences\" to a fixed length\n",
        "# max_len = 10\n",
        "# x = pad_sequences(x, maxlen=max_len)\n",
        "# y = pad_sequences(y, maxlen=max_len)\n",
        "\n",
        "# Convert y to categorical\n",
        "\n",
        "y = to_categorical(y, num_classes=ip_dim)\n",
        "y[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bxKPYKcI050",
        "outputId": "3d03d082-4808-46c9-93e9-e69c218def0c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Embedding\n",
        "model=Sequential()\n",
        "model.add(Embedding(ip_dim,10,input_length=3))\n",
        "model.add(LSTM(1000,return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(ip_dim,activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRyr4-HZI38A",
        "outputId": "6de87ac9-4494-42fe-8ed9-269b6009eac0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 10)             67460     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6746)              6752746   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19869206 (75.80 MB)\n",
            "Trainable params: 19869206 (75.80 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "pqtEau3lI6UM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It gives single predicted words\n",
        "def predict_nxt_word(txt):\n",
        "#\n",
        "\n",
        "    tok_text = tokenizer.texts_to_sequences([ txt])\n",
        "#     print(tok_text)\n",
        "    preds = model.predict(np.array(tok_text), verbose=0)[0]\n",
        "    next_word = tokenizer.sequences_to_texts([[np.argmax(preds)]])[0]\n",
        "    return next_word"
      ],
      "metadata": {
        "id": "3hnR9vc_JCcK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It gives 5 predicted words\n",
        "\n",
        "def predict_nxt_words1(txt, k=5):\n",
        "    tok_text = tokenizer.texts_to_sequences([txt])\n",
        "    preds = model.predict(np.array(tok_text), verbose=0)[0]\n",
        "    top_k_indices = preds.argsort()[-k:][::-1]\n",
        "    top_k_words = [tokenizer.sequences_to_texts([[i]])[0] for i in top_k_indices]\n",
        "    return top_k_words"
      ],
      "metadata": {
        "id": "IlQ7NYyDJE74"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}